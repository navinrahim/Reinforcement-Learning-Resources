{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning Lab Notebook\n",
    "\n",
    "In this notebook, you will learn the basics applying the Q-Learning algorithm to simple environments in OpenAI Gym.  By the end of the notebook, you will have a working Q-Learning agent that can balance a CartPole as well as solve other simple control problems in alternate environments.\n",
    "\n",
    "## Sections\n",
    "\n",
    "1. <a href=#rl>Basic concept of reinforcement learning (RL)</a>\n",
    "- <a href=#cp>CartPole environment from OpenAI Gym</a>\n",
    "- <a href=#random>CartPole with a random agent</a>\n",
    "- <a href=#qlcode>Q-Learning Implementation in Python</a>\n",
    "- <a href=#qlagent>CartPole with a Q-Learning agent</a>\n",
    "- <a href=#mtn>MountainCar environment from OpenAI Gym</a>\n",
    "\n",
    "## Requirements\n",
    "* OpenAI Gym[classic_control] 0.7.4 or higher \n",
    "* Python 3.5 or higher\n",
    "* CUDA8.0 enabled GPU\n",
    "\n",
    "Portions of this notebook have been borrowed from the Nvidia Deep Learning Institute (DLI) hands-on labs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Reinforcement learning <a name='rl' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem setting\n",
    "\n",
    "Unlike standard supervised learning in machine learning, reinforcement learning is about sequential decision making of an agent, which takes actions to maximize cumulative reward, by interacting with environments (problems).\n",
    "\n",
    "<img src=\"image/rl.png\" width=\"200\">\n",
    "\n",
    "After observing the current state $s_t$ and reward $r_t$, agent will decide which action to take $a_t$.\n",
    "\n",
    "For clarity, here we assume that actions are discrete, and reward is a scalar value. Multi-dimensional state $s_t$ can be either discrete or continuous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action-value function approximation\n",
    "\n",
    "There are multiple strategies in RL on how to represent the policy, the behavior mechanism inside the agent and how to optimize it.\n",
    "\n",
    "In most of this hands-on session, we just consider action-value function approximation, in which the expected cumulative reward for all future steps is represented as a function $Q(s, a)$ for pairs of state $s$ and following action $a$.\n",
    "\n",
    "By learning a good approximation of optimal action-value function $Q(s, a)$, by taking action $a_t$ which maximizes the learned Q-value $Q(s_t, a_t)$ given $s_t$ at each time step $t$, the optimal policy should be realized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Example - CartPole <a name='cp' />\n",
    "\n",
    "In this section, we introduce a control problem named CartPole and describe the way it is handled as a RL problem.  Before looking closer at the CartPole environment, we'll import the packages we need and define a visualization function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation: Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install packages in Udacity Workspaces\n",
    "# xvfb-run -s \"-screen 0 1400x900x24\" bash && export DISPLAY=:0\n",
    "!python -m pip install pyvirtualdisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment for Udacity server using xvfb\n",
    "from pyvirtualdisplay import Display\n",
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The typical imports\n",
    "import gym\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# visualization helpers\n",
    "from IPython import display\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Set logger level\n",
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CartPole Environment\n",
    "\n",
    "As an example of classical RL problem, we use CartPole-v0 from [OpenAI Gym](https://gym.openai.com/), which contains two-dimensional physics simulator of a black cart and a yellow pole. It is a kind of inverted pendulum.\n",
    "\n",
    "<img src=\"image/cartpole.png?\" width=\"450\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following overview of CartPole-v0 is taken from [Gym's Wiki page](https://github.com/openai/gym/wiki/CartPole-v0)\n",
    "\n",
    "#### Description\n",
    "By an un-actuated joint, a pole is attached to a cart, which moves along a frictionless track. The pendulum starts upright, and the goal is to prevent it from falling over by increasing and reducing the cart's velocity.\n",
    "\n",
    "#### Observation\n",
    "Type: Box(4)\n",
    "\n",
    "Num | Observation | Min | Max\n",
    "---|---|---|---\n",
    "0 | Cart Position | -2.4 | 2.4\n",
    "1 | Cart Velocity | -Inf | Inf\n",
    "2 | Pole Angle | ~ -41.8&deg; | ~ 41.8&deg;\n",
    "3 | Pole Velocity At Tip | -Inf | Inf\n",
    "\n",
    "#### Actions\n",
    "Type: Discrete(2)\n",
    "\n",
    "Num | Action\n",
    "--- | ---\n",
    "0 | Push cart to the left\n",
    "1 | Push cart to the right\n",
    "\n",
    "Note: The amount the velocity is reduced or increased is not fixed as it depends on the angle the pole is pointing. This is because the center of gravity of the pole increases the amount of energy needed to move the cart underneath it\n",
    "\n",
    "#### Conditions for episode termination \n",
    "1. Pole Angle is more than ±20.9°\n",
    "2. Cart Position is more than ±2.4 (center of the cart reaches the edge of the display)\n",
    "3. Episode length is greater than 200\n",
    "\n",
    "#### Reward\n",
    "Reward is 1 for every step taken, including the termination step\n",
    "\n",
    "#### Starting state\n",
    "All observations are assigned a uniform random value between ±0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Gym's environment works\n",
    "\n",
    "Gym's environment (problem) has a unified interface with the following three steps.\n",
    "\n",
    "#### 1. Create environment specified by name\n",
    "```python\n",
    "env = gym.make('ENV_NAME')\n",
    "```\n",
    "#### 2. Initialize environment\n",
    "\n",
    "```python\n",
    "env.reset()\n",
    "```\n",
    "#### 3. Take action and observe reward & next state\n",
    "```python\n",
    "observation, reward, is_finished, info = env.step(action)\n",
    "```\n",
    "where\n",
    "  - obs: a next observation\n",
    "  - reaward: a scalar reward\n",
    "  - is_finished: a boolean value indicating whether the current state is terminal or not\n",
    "  - info: additional information\n",
    "\n",
    "By interacting the environment, a reinforcement learning agent learns how to optimize its strategy for maximizing cumulative reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution: How the environment works\n",
    "\n",
    "Create the CartPole environment and observe the initial state and the result of an action.  In this case a random action has been selected by sampling the action space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-03-05 18:38:37,493] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation space: Box(4,)\n",
      "action space: Discrete(2)\n",
      "initial observation: [0.0028074  0.02312983 0.01636997 0.00287255]\n",
      "random action: 0\n",
      "next observation: [ 0.00326999 -0.17222302  0.01642742  0.30067511]\n",
      "reward: 1.0\n",
      "is_finished: False\n",
      "info: {}\n"
     ]
    }
   ],
   "source": [
    "# Create  environment of CartPole-v0\n",
    "env = gym.make('CartPole-v0')\n",
    "print('observation space:', env.observation_space)\n",
    "print('action space:', env.action_space)\n",
    "\n",
    "observation = env.reset()\n",
    "# env.render(mode='rgb_array', close=True)\n",
    "print('initial observation:', observation)\n",
    "\n",
    "action = env.action_space.sample() # Select random action\n",
    "print('random action:', action)\n",
    "observation, reward, is_finished, info = env.step(action)\n",
    "print('next observation:', observation)\n",
    "print('reward:', reward)\n",
    "print('is_finished:', is_finished)\n",
    "print('info:', info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. CartPole with the Random agent <a name='random' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class: Random agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copied from gym examples - a model for our agent\n",
    "class RandAgent(object):\n",
    "    \"\"\"The world's simplest agent!\"\"\"\n",
    "    def __init__(self, action_space):\n",
    "        self.action_space = action_space\n",
    "\n",
    "    def act(self, observation, reward, done=None, mode=None):\n",
    "        return self.action_space.sample()\n",
    "    \n",
    "    def init_episode(self, observation):\n",
    "        # provided for compatibility with general learner\n",
    "        return self.action_space.sample()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition: Learner for agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This learner provides a general interface for agents and environments \n",
    "# optionally visualize within a Jupyter Notebook when visualize_plt is True\n",
    "def learner(agent=None, env_id='CartPole-v0', episodes=100, max_length = 100, init_reward=0, \n",
    "            ignore_done=False, visualize_plt=True, mode=None):\n",
    "    # load the environment \n",
    "    env = gym.make(env_id)\n",
    "    # set the agent to random if none provided\n",
    "    if agent is None:\n",
    "        agent = RandAgent(env.action_space)\n",
    "\n",
    "    # each episode runs until it is observed as finished, or exceeds max_length in time steps\n",
    "    episode_count = episodes\n",
    "    done = False\n",
    "    n_steps = np.zeros((episode_count,))\n",
    "\n",
    "    # run the episodes - use tqdm to track in the notebook\n",
    "    for i in tqdm(range(episode_count), disable=visualize_plt):\n",
    "        \n",
    "        # Initialize environment for each episode\n",
    "        ob = env.reset()  \n",
    "        reward = init_reward\n",
    "        if visualize_plt:\n",
    "            img = plt.imshow(env.render(mode='rgb_array')) # only call this once, only for jupyter\n",
    "        \n",
    "        # initialize the agent\n",
    "        agent.init_episode(ob)\n",
    "        n_steps[i]=max_length\n",
    "        \n",
    "        # run the steps in each epsisode\n",
    "        for t in range(max_length):\n",
    "            # render the environment\n",
    "            if visualize_plt:\n",
    "                img.set_data(env.render(mode='rgb_array')) # just update the data\n",
    "                plt.axis('off')\n",
    "                display.display(plt.gcf())\n",
    "                display.clear_output(wait=True)\n",
    "            else:\n",
    "                env.render()\n",
    "            \n",
    "            # get agent's action\n",
    "            action = agent.act(ob, reward, mode=mode)\n",
    "            # take the action and get reward and updated observation\n",
    "            ob, reward, done, _ = env.step(action)\n",
    "            \n",
    "            # terminate the steps if the problem is done\n",
    "            if done and not ignore_done:\n",
    "                n_steps[i] = t\n",
    "                break\n",
    "            if done and ignore_done and env_id=='MountainCar-v0' and ob[0]>= 0.5:\n",
    "                # special case MountainCar\n",
    "                # if have achieved the goal, then quit but otherwise keep going\n",
    "                print(\"Episode {} done at step {}\".format(i,t))\n",
    "                print(\"Observations {}, Reward {}\".format(ob, reward))\n",
    "                n_steps[i] = t\n",
    "                break\n",
    "    env.render(close=True)\n",
    "    return n_steps  # stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution: Run the experiment with the random agent\n",
    "\n",
    "Before introducing an RL agent, you can just repeat random actions to see how the environment changes over time. Here you repeat the episode for 10 times.  The number of steps before the cart fails are recorded for each episode.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAD8CAYAAAB9y7/cAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAABG9JREFUeJzt3MFNG1EUQNGZyE2kjlBG6rDboA1cR8oIdVCGs/EiYIiECIx93zmSFyBZegu4+vpv7PV0Oi0A9HzbegAAPofAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPEDUbusBznycFuDS+pE3O8EDRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPEDUbusB4Fo8Hg/Pfv6xf9hoEvg/nODhDY/Hw0X04ZYIPECUwMM/uKbhlgk8QJTAw3K5YIUCgQeIEniAKIGHN1iwcusEHiBK4BnPgpUqgQeIEniAKIGHV1iwUiDwAFECz2gWrJQJPECUwANECTy8YMFKhcADRAk8Y1mwUifw8BfXM5QIPECUwANECTxAlMAzkgUrEwg8nFmwUiPwAFECzziuZ5hC4AGiBB4gSuBhsWClSeABogSeUSxYmUTgAaIEHiBK4BnPgpUqgQeIEniAKIFnDE/QMI3AA0QJPECUwDOaJ2goE3hGcP/ORAIPECXwAFECDxAl8IxlwUqdwANECTx5nqBhKoEHiBJ4gCiBZyQLViYQeIAogSfttQWr0ztTCDxAlMADRAk8WZ5/ZzqBB4gSeIAogedmrOv6rtdr7g7HD70fbonAA0Ttth4APsPvh/3y62n/7Hc/vx83mga24QRP0su4w0QCzxiizzQCzxj393dbjwBfSuBJct8Oy7KeTqetZ1iWZbmKIbhuX/3o4pX8bzDbh/7oneABogQeIErgAaIEHiBK4AGiBB4gSuABogQeIErgAaJ8XTA3wydL4X2c4AGiBB4gSuABogQeIErgAaIEHiBK4AGiBB4gSuABogQeIErgAaIEHiBK4AGiBB4gSuABogQeIErgAaIEHiBK4AGiBB4gSuABogQeIErgAaIEHiBK4AGiBB4gSuABogQeIErgAaIEHiBK4AGiBB4gSuABogQeIErgAaIEHiBK4AGiBB4gSuABogQeIErgAaIEHiBK4AGiBB4gSuABonZbD3C2bj0AQI0TPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QNQfSpJHDLjl1nEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11a68f588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_episodes = 10\n",
    "max_length = 200\n",
    "steps = learner(episodes=num_episodes, max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum step count in 10 episodes: 16.0\n",
      "Average step count in 10 episodes: 25.5\n",
      "Maximum step count in 10 episodes: 38.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Minimum step count in {} episodes: {}\".format(num_episodes, np.min(steps)))\n",
    "print(\"Average step count in {} episodes: {}\".format(num_episodes, np.mean(steps)))\n",
    "print(\"Maximum step count in {} episodes: {}\".format(num_episodes, np.max(steps)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Traditional RL = Q-learning <a name='qlcode' />\n",
    "\n",
    "Q-learning is one of the most popular approaches in reinforcement learning, to determine which action is optimal on each state.\n",
    "\n",
    "At each time step $t$, after taking an action $a_t$ and given reward $r_{t+1}$ and next state $s_{t+1}$, the old Q-value $Q(s_t, a_t)$ is updated by the following rule. \n",
    "\n",
    "<img src=\"image/q-learning.svg\" width=\"800\">\n",
    "\n",
    "It is not necessary to follow the details of math but the intuition is: the Q-value should be updated by taking into account the difference between the old value and the sum of the actual reward and the discounted estimated future value obtained by next action, by the factor of learning rate $\\alpha_t$.\n",
    "\n",
    "Q-learning is an iterative algorithm, so Q-values are used to take actions and then gradually improved over time.\n",
    "\n",
    "To make the good balance between exploration (random action) and exploitation (estimated best action), $\\epsilon$-greedy strategy is typically used with Q-learning in which where the agent take random actions with probability $\\epsilon$  and current best action otherwise.\n",
    "\n",
    "The simplest way to implement Q-Learning is to build a table, named Q-table, to store the Q-value for each discrete state and action combination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class: Q-learning agent\n",
    "\n",
    "Here we define the QLearningAgent class, in which the observation space is discretized into q-tables of size 20,000 =  ((that of bins (9)+1) ^ numbers of observations (4)) * number of actions (2). Each cell represents current Q-value for each combination of discretized state and possible action.\n",
    "\n",
    "By calling the `act()` method with the current observation (and reward), the agent updates the Q-table and returns the next action.\n",
    "\n",
    "The hyper-parameters for training have been set at default values, but may be modified when called:\n",
    "\n",
    "**`learning_rate` = 0.2**\n",
    "* the new information replaces 20% of the old information at each step\n",
    "\n",
    "**`discount_factor` = 1.0** \n",
    "* the agent will strive for a long-term high reward because the value includes expected future rewards\n",
    "\n",
    "**`exploration_rate` = 0.5**\n",
    "* the agent will choose a random action, i.e. \"explore\", 50% of the time\n",
    "\n",
    "**`exploration_decay_rate` = 0.99**\n",
    "* the probability of exploration will be reduced by 1% at the start of each episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    def __init__(self, \n",
    "                 learning_rate = 0.2, discount_factor = 1.0,\n",
    "                 exploration_rate = 0.5, exploration_decay_rate = 0.99,\n",
    "                 n_bins = 9, n_actions = 2, splits=None):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.exploration_rate = exploration_rate # initial epsilon\n",
    "        self.exploration_decay_rate = exploration_decay_rate # decay factor for epsilon\n",
    "        self.n_bins = n_bins\n",
    "        self.n_actions = n_actions\n",
    "        self.splits = splits\n",
    "        self.state = None\n",
    "        self.action = None\n",
    "        \n",
    "        if self.splits is None: #CartPole default\n",
    "            self.splits = [\n",
    "                # Position\n",
    "                np.linspace(-2.4, 2.4, self.n_bins)[1:-1],\n",
    "                # Velocity\n",
    "                np.linspace(-3.5, 3.5, self.n_bins)[1:-1],\n",
    "                # Angle.\n",
    "                np.linspace(-0.5, 0.5, self.n_bins)[1:-1],\n",
    "                # Tip velocity\n",
    "                np.linspace(-2.0, 2.0, self.n_bins)[1:-1]\n",
    "            ]\n",
    "        \n",
    "        # Create Q-Table\n",
    "        num_states = (self.n_bins+1) ** len(self.splits)\n",
    "        self.q_table = np.zeros(shape=(num_states, self.n_actions))\n",
    "\n",
    "    # Turn the observation into integer state\n",
    "    def set_state(self, observation):\n",
    "        state = 0\n",
    "        for i, column in enumerate(observation):\n",
    "            state +=  np.digitize(x=column, bins=self.splits[i]) * ((self.n_bins + 1) ** i)\n",
    "        return state\n",
    "\n",
    "    # Initialize for each episode\n",
    "    def init_episode(self, observation):\n",
    "        # Gradually decrease exploration rate\n",
    "        self.exploration_rate *= self.exploration_decay_rate\n",
    "\n",
    "        # Decide initial action\n",
    "        self.state = self.set_state(observation)\n",
    "        return np.argmax(self.q_table[self.state])\n",
    "\n",
    "    # Select action and update\n",
    "    def act(self, observation, reward=None, done=None, mode='train'):\n",
    "        next_state = self.set_state(observation)\n",
    "        \n",
    "        if mode == 'test':\n",
    "            # Test mode \n",
    "            next_action = np.argmax(self.q_table[next_state])\n",
    "        else:\n",
    "            # Train mode by default\n",
    "            # Train by updating Q-Table based on current reward and 'last' action.\n",
    "            self.q_table[self.state, self.action] += self.learning_rate * \\\n",
    "                (reward + self.discount_factor * max(self.q_table[next_state, :]) - self.q_table[self.state, self.action])\n",
    "            # Exploration or exploitation\n",
    "            do_exploration = (1 - self.exploration_rate) < np.random.uniform(0, 1)\n",
    "            if do_exploration:\n",
    "                #  Exploration\n",
    "                next_action = np.random.randint(0, self.n_actions)\n",
    "            else:\n",
    "                # Exploitation\n",
    "                next_action = np.argmax(self.q_table[next_state])\n",
    "\n",
    "        self.state = next_state\n",
    "        self.action = next_action\n",
    "        return next_action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. CartPole with the Q-Learning agent <a name='qlagent' />\n",
    "\n",
    "### Preparation: Initialize the Q-learning agent and training parameters\n",
    "\n",
    "Beginning from an empty Q-table, QLearningAgent tries to learn $Q(s_t, a_t)$ by Q-learning with $\\epsilon$-greedy strategy in 50 episodes. The result gif shows the agent gradually learns from trials and errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the agent\n",
    "q_agent = QLearningAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 50\n",
    "max_length = 200\n",
    "initial_reward = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution: Train the Q-learning agent\n",
    "\n",
    "Each time the learner cell is executed, the agent improves its policy.  To start over from scratch with the agent, go back and execute the cell that instantiates the agent with `q_agent = QLearningAgent()` \n",
    "\n",
    "The result statistics of average and maximum steps achieved during the episodes shows that over time the agent is able to improve its behavior.  We can see this in the visualization as the CartPole is able to stay verticle for longer periods of time after more training episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-03-05 18:45:11,448] Making new env: CartPole-v0\n",
      "100%|██████████| 50/50 [00:22<00:00,  2.23it/s]\n"
     ]
    }
   ],
   "source": [
    "# train the agent - execute this cell as many times as you wish\n",
    "# set the visualize_plt flag to True to see the cart in the notebook.  \n",
    "# note that this will run slower if visualized\n",
    "steps = learner(agent=q_agent, episodes=num_episodes, max_length = max_length, \n",
    "                init_reward=initial_reward, visualize_plt=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum step count in 50 episodes: 9.0\n",
      "Average step count in 50 episodes: 25.92\n",
      "Maximum step count in 50 episodes: 131.0\n",
      "Q-table size:  20000\n",
      "Q-table nonzero count:  126\n"
     ]
    }
   ],
   "source": [
    "print(\"Minimum step count in {} episodes: {}\".format(num_episodes, np.min(steps)))\n",
    "print(\"Average step count in {} episodes: {}\".format(num_episodes, np.mean(steps)))\n",
    "print(\"Maximum step count in {} episodes: {}\".format(num_episodes, np.max(steps)))\n",
    "print(\"Q-table size: \", q_agent.q_table.size)\n",
    "print(\"Q-table nonzero count: \", np.count_nonzero(q_agent.q_table))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution: Test the trained agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAD8CAYAAAB9y7/cAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAABHZJREFUeJzt3MFNW0EUQFF+5CZSRygjdUAbtIHrSBmkjpThbCxFwQbZspL/fd85OxCGWdhXw7yxl8Ph8ABAz5e1FwDAvyHwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxC1W3sBR95OC3BqueXBdvAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxC1W3sBsCU/989/ff3t6XWllcDt7ODhE++DD/dE4AGiBB6O7NapEXiAKIEHiBJ4+IRbNNwzgQeIEnh4MGClSeABogQeIErg4QMGrNw7gQeIEnjGM2ClSuABogQeIErg4QwDVgoEHiBK4BnNgJUygQeIEniAKIEHiBJ4eMcNGioEHiBK4Bnr3A0au3dKBB4gSuABogQeIErgAaIEnpEMWJlA4AGiBJ5xfMAYUwg8QJTAA0QJPECUwMODGzQ0CTyjGLAyicADRAk8QJTAA0QJPOMZsFIl8ABRAs8YbtAwjcADRAk8QJTAM5oBK2UCDxAl8IxgwMpEAg8QJfAAUQLPWAas1Ak8QJTAk2fAylQCDxAl8ABRAs9IBqxMIPAAUQIPECXwpJ27QeN4hikEHiBqt/YC4BrLslz8s2+vTzf9jsPhcPHfgi2ygweIsoMn7cevP7v471/3K64E/r9lI/+GbmIRbN81RzQvL29nvvd48eM38tpgtsuf8Gc4ogGIEniSPhqwwiQCT9Lj8/7kzN0ZPNM4g+euXHMGf6uNvDaYzRk8AKcEHiBK4AGiBB4gSuABogQeIErgAaIEHiBK4AGifFwwd8W7S+FydvAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFG7tRdwtKy9AIAaO3iAKIEHiBJ4gCiBB4gSeIAogQeIEniAKIEHiBJ4gCiBB4gSeIAogQeIEniAKIEHiBJ4gCiBB4gSeIAogQeIEniAKIEHiBJ4gCiBB4gSeICo30poT5FEFDiNAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11a965278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Testing the agent - run this smaller sampling after the agent is achieving success and NOT exploring\n",
    "\n",
    "num_episodes = 5\n",
    "max_length = 200\n",
    "initial_reward = 1\n",
    "steps = learner(agent=q_agent, episodes=num_episodes, max_length = max_length,\n",
    "            init_reward=initial_reward, mode='test') # set mode to 'test' to avoid exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum step count in 5 episodes: 9.0\n",
      "Average step count in 5 episodes: 11.2\n",
      "Maximum step count in 5 episodes: 14.0\n",
      "Q-table size:  20000\n",
      "Q-table nonzero count:  100\n"
     ]
    }
   ],
   "source": [
    "print(\"Minimum step count in {} episodes: {}\".format(num_episodes, np.min(steps)))\n",
    "print(\"Average step count in {} episodes: {}\".format(num_episodes, np.mean(steps)))\n",
    "print(\"Maximum step count in {} episodes: {}\".format(num_episodes, np.max(steps)))\n",
    "print(\"Q-table size: \", q_agent.q_table.size)\n",
    "print(\"Q-table nonzero count: \", np.count_nonzero(q_agent.q_table))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Example: MountainCar-v0 <a name='mtn' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenAI Gym is full of environments to try with varying difficulties and challenges.  Start by reviewing the information found in the [wiki](https://github.com/openai/gym/wiki/Table-of-environments).  Here's some information provided about the [MountainCar-v0 environment](https://github.com/openai/gym/wiki/MountainCar-v0):\n",
    "\n",
    "<img src=\"image/mtncar.png?\" width=\"450\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Description\n",
    "Get an under powered car to the top of a hill (top = 0.5 position).\n",
    "\n",
    "#### Observation\n",
    "Type: Box(2)\n",
    "\n",
    "Num | Observation | Min | Max\n",
    "---|---|---|---\n",
    "0 | Position | -1.2 | 0.6\n",
    "1 | Velocity | -0.07 | 0.07\n",
    "\n",
    "#### Actions\n",
    "Type: Discrete(3)\n",
    "\n",
    "Num | Action\n",
    "--- | ---\n",
    "0 | push left\n",
    "1 | no push\n",
    "2 | push right\n",
    "\n",
    "#### Reward\n",
    "-1 for each time step, until the goal position of 0.5 is reached. As with MountainCarContinuous v0, there is no penalty for climbing the left hill, which upon reached acts as a wall.\n",
    "\n",
    "#### Starting state\n",
    "Random position from -0.6 to -0.4 with no velocity.\n",
    "\n",
    "#### Episode termination\n",
    "The episode ends when you reach 0.5 position, or if 200 iterations are reached.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution: MountainCar with the Random agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:29<00:00,  9.78s/it]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x119800550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# try mountain car with the Random agent (default)\n",
    "steps = learner(env_id='MountainCar-v0', episodes=3, max_length = 200, init_reward=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation: MountainCar with the Q-Learning agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running the Q-Learning agent, we need to consider how the environment setup will affect the learning for our agent.  The MountainCar-v0 problem is a hard one because the agent can't really learn what actions are helpful until it actually climbs the hill successfully.  As long as that goal has not been met, all the episodes will have an accumulated reward of -1 for each timestep, or -200, since the number of steps allowed is capped.  Therefore, it's important that the car explores as many avenues as possible to solve the problem.  As a help for this notebook, we can extend the number of timesteps for the learner by ignoring the \"done\" signal and setting a higher number for the `max_length` parameter in learner. A special case provision has been included in the learner for this operation specifically for MountainCar-v0\n",
    "\n",
    "The Q-Learning agent also needs a way to discretize the observations for MountainCar, just as it did for the CartPole problem.  Although the agent itself will use the same basic iterative algorithm, we need to change the setup a bit.  This can be accomplished by setting up a different table of \"splits\".\n",
    "\n",
    "Whereas in CartPole, the split was:\n",
    "``` python\n",
    "\n",
    "    splits = [\n",
    "    # Position\n",
    "    np.linspace(-2.4, 2.4, n_bins)[1:-1],\n",
    "    # Velocity\n",
    "    np.linspace(-3.5, 3.5, n_bins)[1:-1],\n",
    "    # Angle.\n",
    "    np.linspace(-0.5, 0.5, n_bins)[1:-1],\n",
    "    # Tip velocity\n",
    "    np.linspace(-2.0, 2.0, n_bins)[1:-1]\n",
    "    ]\n",
    " ```\n",
    " \n",
    "MountainCar only has two observation values:\n",
    "``` python\n",
    "    splits = [\n",
    "    # Position\n",
    "    np.linspace(-1.2, 0.6, n_bins)[1:-1],\n",
    "    # Velocity\n",
    "    np.linspace(-0.07, 0.07, n_bins)[1:-1],\n",
    "    ]\n",
    "```\n",
    "In addition, the MountainCar problem has three actions whereas the CarPole only had two, so the agent will need to knw that . The Q-Learning agent defined previously has a provision for passing all the parameters it uses including the number of actions and splits.  Go ahead and create a split table now for the MountainCar Q-Learning agent and define the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO define the number of bins `n_bins` and a list name `splits_mtncar` for use by the Q-Learning agent.\n",
    "# The n_bins used for CartPole was 9, but feel free to experiment with this number\n",
    "# n_bins = \n",
    "# splits_mtncar = \n",
    "\n",
    "\n",
    "# ANSWER\n",
    "n_bins = 20\n",
    "splits_mtncar = [\n",
    "    # Position\n",
    "    np.linspace(-1.2, 0.6, n_bins)[1:-1],\n",
    "    # Velocity\n",
    "    np.linspace(-0.07, 0.07, n_bins)[1:-1],\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO instantiate a QLearningAgent named q_agent_mtncar\n",
    "# you may want to tweak the hyper-parameters, such as the exploration rate, to increase\n",
    "#    the agent's chances of climbing the hill\n",
    "# q_agent_mtncar = \n",
    "\n",
    "\n",
    "# ANSWER\n",
    "q_agent_mtncar = QLearningAgent(learning_rate = 0.2, discount_factor = 0.9,\n",
    "                                exploration_rate = 0.8,\n",
    "                                n_actions = 3, n_bins = n_bins, splits = splits_mtncar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-03-06 07:37:08,791] Making new env: MountainCar-v0\n",
      "  2%|▏         | 1/50 [00:11<09:18, 11.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 done at step 681\n",
      "Observations [0.50576969 0.02505412], Reward -1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2/50 [00:18<07:34,  9.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 done at step 452\n",
      "Observations [0.50467235 0.00730901], Reward -1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 3/50 [00:26<06:52,  8.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2 done at step 442\n",
      "Observations [0.52298422 0.02406063], Reward -1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 4/50 [00:35<06:48,  8.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3 done at step 550\n",
      "Observations [0.50795571 0.01929939], Reward -1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 5/50 [00:43<06:34,  8.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4 done at step 496\n",
      "Observations [0.51619329 0.0172518 ], Reward -1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 6/50 [00:59<07:16,  9.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 5 done at step 939\n",
      "Observations [0.5067408  0.00836884], Reward -1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 7/50 [01:10<07:14, 10.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 6 done at step 675\n",
      "Observations [0.50069823 0.01587882], Reward -1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 8/50 [01:21<07:08, 10.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 7 done at step 653\n",
      "Observations [0.51187526 0.0235919 ], Reward -1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 9/50 [01:36<07:18, 10.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 8 done at step 873\n",
      "Observations [0.51487183 0.01987083], Reward -1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 10/50 [01:43<06:54, 10.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 9 done at step 449\n",
      "Observations [0.50004679 0.00199781], Reward -1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 11/50 [01:51<06:36, 10.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10 done at step 481\n",
      "Observations [0.50006355 0.01249839], Reward -1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 12/50 [02:05<06:36, 10.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 11 done at step 814\n",
      "Observations [0.50342189 0.01136546], Reward -1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 13/50 [02:10<06:12, 10.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 12 done at step 327\n",
      "Observations [0.50143549 0.01566589], Reward -1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 14/50 [02:18<05:56,  9.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 13 done at step 463\n",
      "Observations [0.50088983 0.0050245 ], Reward -1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 15/50 [02:30<05:51, 10.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 14 done at step 714\n",
      "Observations [0.50410506 0.0169259 ], Reward -1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 16/50 [02:52<06:06, 10.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 15 done at step 1326\n",
      "Observations [0.50053545 0.02081159], Reward -1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 17/50 [03:04<05:57, 10.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 16 done at step 691\n",
      "Observations [0.51622622 0.0228809 ], Reward -1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 18/50 [03:16<05:48, 10.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 17 done at step 716\n",
      "Observations [0.5084945  0.02228761], Reward -1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 19/50 [03:23<05:32, 10.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 18 done at step 454\n",
      "Observations [0.51922718 0.02214996], Reward -1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 20/50 [03:30<05:15, 10.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 19 done at step 393\n",
      "Observations [0.50846272 0.01580767], Reward -1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 21/50 [03:48<05:14, 10.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 20 done at step 1070\n",
      "Observations [0.52215211 0.02394041], Reward -1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 22/50 [03:55<04:59, 10.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 21 done at step 432\n",
      "Observations [0.50863285 0.01791393], Reward -1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 23/50 [04:01<04:43, 10.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 22 done at step 392\n",
      "Observations [0.50618345 0.01405835], Reward -1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 24/50 [04:07<04:28, 10.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 23 done at step 355\n",
      "Observations [0.50046    0.00962544], Reward -1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 25/50 [04:16<04:16, 10.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 24 done at step 511\n",
      "Observations [0.50888628 0.01891644], Reward -1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 26/50 [04:21<04:01, 10.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 25 done at step 318\n",
      "Observations [0.51748462 0.01869037], Reward -1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 27/50 [04:26<03:47,  9.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 26 done at step 322\n",
      "Observations [0.51182131 0.02173939], Reward -1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 28/50 [04:37<03:38,  9.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 27 done at step 647\n",
      "Observations [0.50604518 0.01975056], Reward -1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 29/50 [04:44<03:25,  9.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 28 done at step 389\n",
      "Observations [0.51770659 0.02305719], Reward -1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 30/50 [04:53<03:15,  9.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 29 done at step 527\n",
      "Observations [0.50648643 0.02758079], Reward -1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 31/50 [04:57<03:02,  9.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 30 done at step 275\n",
      "Observations [0.50840479 0.01861226], Reward -1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 32/50 [05:02<02:50,  9.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 31 done at step 316\n",
      "Observations [0.52314346 0.02403143], Reward -1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 33/50 [05:13<02:41,  9.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 32 done at step 603\n",
      "Observations [0.52488056 0.0273299 ], Reward -1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 34/50 [05:21<02:31,  9.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 33 done at step 513\n",
      "Observations [0.5150931  0.01557987], Reward -1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 35/50 [05:34<02:23,  9.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 34 done at step 765\n",
      "Observations [0.51441739 0.01901121], Reward -1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 36/50 [05:40<02:12,  9.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 35 done at step 369\n",
      "Observations [0.5048083  0.02615862], Reward -1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 37/50 [05:48<02:02,  9.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 36 done at step 468\n",
      "Observations [0.5226023  0.02734901], Reward -1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 38/50 [05:56<01:52,  9.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 37 done at step 499\n",
      "Observations [0.51590655 0.02266197], Reward -1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 39/50 [06:03<01:42,  9.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 38 done at step 425\n",
      "Observations [0.522494   0.02575842], Reward -1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 40/50 [06:09<01:32,  9.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 39 done at step 341\n",
      "Observations [0.50398394 0.01276547], Reward -1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 41/50 [06:24<01:24,  9.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 40 done at step 891\n",
      "Observations [0.50389029 0.0070148 ], Reward -1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 42/50 [06:31<01:14,  9.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 41 done at step 457\n",
      "Observations [0.51050759 0.01208402], Reward -1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 43/50 [06:38<01:04,  9.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 42 done at step 398\n",
      "Observations [0.5147545  0.01875282], Reward -1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 44/50 [06:45<00:55,  9.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 43 done at step 416\n",
      "Observations [0.50772959 0.02348888], Reward -1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 45/50 [06:52<00:45,  9.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 44 done at step 398\n",
      "Observations [0.5090438  0.02923246], Reward -1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 46/50 [07:02<00:36,  9.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 45 done at step 637\n",
      "Observations [0.51498697 0.02343245], Reward -1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 47/50 [07:10<00:27,  9.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 46 done at step 463\n",
      "Observations [0.5059727  0.01156899], Reward -1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 48/50 [07:18<00:18,  9.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 47 done at step 469\n",
      "Observations [0.51359731 0.01911369], Reward -1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 49/50 [07:24<00:09,  9.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 48 done at step 362\n",
      "Observations [0.51933087 0.03031587], Reward -1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [07:31<00:00,  9.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 49 done at step 422\n",
      "Observations [0.50156094 0.0281612 ], Reward -1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run the agent long enough for it to achieve success within 500 steps\n",
    "# Feel free to modify any of these parameters\n",
    "num_episodes = 50\n",
    "max_length = 2000\n",
    "steps = learner(agent=q_agent_mtncar, env_id='MountainCar-v0',\n",
    "                        episodes=num_episodes, max_length = max_length, init_reward=0,\n",
    "                        ignore_done=True, visualize_plt=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum step count in 50 episodes: 275.0\n",
      "Average step count in 50 episodes: 540.78\n",
      "Maximum step count in 50 episodes: 1326.0\n",
      "Q-table size:  1323\n",
      "Q-table nonzero count:  754\n"
     ]
    }
   ],
   "source": [
    "print(\"Minimum step count in {} episodes: {}\".format(num_episodes, np.min(steps)))\n",
    "print(\"Average step count in {} episodes: {}\".format(num_episodes, np.mean(steps)))\n",
    "print(\"Maximum step count in {} episodes: {}\".format(num_episodes, np.max(steps)))\n",
    "print(\"Q-table size: \", q_agent_mtncar.q_table.size)\n",
    "print(\"Q-table nonzero count: \", np.count_nonzero(q_agent_mtncar.q_table))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution: Test the trained agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2 done at step 182\n",
      "Observations [0.52397132 0.03085815], Reward -1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x119800ba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Testing the agent - run this smaller sampling after the agent is achieving success\n",
    "num_episodes = 3\n",
    "max_length = 500\n",
    "steps = learner(agent=q_agent_mtncar, env_id='MountainCar-v0',\n",
    "                        episodes=num_episodes, max_length = max_length, init_reward=0,\n",
    "                        ignore_done=True, visualize_plt=True, mode='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum step count in 3 episodes: 182.0\n",
      "Average step count in 3 episodes: 228.66666666666666\n",
      "Maximum step count in 3 episodes: 254.0\n",
      "Q-table size:  1323\n",
      "Q-table nonzero count:  754\n"
     ]
    }
   ],
   "source": [
    "print(\"Minimum step count in {} episodes: {}\".format(num_episodes, np.min(steps)))\n",
    "print(\"Average step count in {} episodes: {}\".format(num_episodes, np.mean(steps)))\n",
    "print(\"Maximum step count in {} episodes: {}\".format(num_episodes, np.max(steps)))\n",
    "print(\"Q-table size: \", q_agent_mtncar.q_table.size)\n",
    "print(\"Q-table nonzero count: \", np.count_nonzero(q_agent_mtncar.q_table))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations!  \n",
    "Now you are on your way to trying other environment problems and RL algorithms.  Feel free to add additional cells here in order to try additional environments in OpenAI Gym."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
